{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JndnmDMp66FL"
   },
   "source": [
    "#### Copyright 2017 Google LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "hMqWDc_m6rUC"
   },
   "outputs": [],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4f3CKqFUqL2-",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lab 2: Training Your First TF Linear Regression Model\n",
    "\n",
    "\n",
    "**Learning Objectives:**\n",
    "* Use pyplot to help visualize the data, the learned model, and how the loss is evolving during training\n",
    "* Learn how to set up the features in TensorFlow to train a model.\n",
    "* Use the `LinearRegressor` class in TensorFlow to predict a real-valued featured based on one real-valued input feature\n",
    "* Visualize the resulting model using pyplot\n",
    "* Evaluate the accuracy of a model's predictions using Root Mean Squared Error (RMSE)\n",
    "* Improve the accuracy of a model by tuning the learning rate and number of training steps\n",
    "\n",
    "There are a lot of things introduced in this lab so carefully look through what is provided and ask if you have questions. Much of what is here will be used throughout this course without the need to modify them.  Others, we will slowly modify as needed to handle the complexities that arise in real-world learning problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "if3EBefJeqKq"
   },
   "source": [
    "## Data Set\n",
    "This lab will use a data set from 1985 Ward's Automotive Yearbook that is part of the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets) under [Automobile Data Set](https://archive.ics.uci.edu/ml/datasets/automobile).  You can find a description of the data at [https://archive.ics.uci.edu/ml/datasets/automobile](https://archive.ics.uci.edu/ml/datasets/automobile). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h94BDSHgeKev"
   },
   "source": [
    "## Imports and basic set-up\n",
    "As in the last lab, we'll import some libraries and set up some options for Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "test": {
      "output": "ignore",
      "timeout": 600
     }
    },
    "colab_type": "code",
    "id": "6CspO1zVopXj",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "import math\n",
    "\n",
    "from IPython import display\n",
    "from matplotlib import cm\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn import learn_io, estimator\n",
    "\n",
    "# This line increases the amount of logging when there is an error.  You can\n",
    "# remove it if you want less logging\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Set the output display to have two digits for decimal places, for display\n",
    "# readability only and limit it to printing 15 rows.\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "pd.options.display.max_rows = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F9TLXouIfYQI"
   },
   "source": [
    "## Load and randomizing the data\n",
    "Load the data using the column names from [Automobile Data Set](https://archive.ics.uci.edu/ml/datasets/automobile). When using SGD (stochastic graident descent) for training it is important that **each batch is a random sample of the data** so that the gradient computed is representative.  While there appears to be no order to this data set, it is always good practice to shuffle the data to be in a random order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "wXuaBqVufYF4"
   },
   "outputs": [],
   "source": [
    "# Provide the names for the columns since the CSV file with the data does\n",
    "# not have a header row.\n",
    "cols = ['symboling', 'losses', 'make', 'fuel-type', 'aspiration', 'num-doors',\n",
    "        'body-style', 'drive-wheels', 'engine-location', 'wheel-base',\n",
    "        'length', 'width', 'height', 'weight', 'engine-type', 'num-cylinders',\n",
    "        'engine-size', 'fuel-system', 'bore', 'stroke', 'compression-ratio',\n",
    "        'horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'price']\n",
    "\n",
    "\n",
    "# Load in the data from a CSV file that is comma seperated.\n",
    "car_data = pd.read_csv('https://storage.googleapis.com/ml_universities/cars_dataset/cars_data.csv',\n",
    "                        sep=',', names=cols, header=None, encoding='latin-1')\n",
    "\n",
    "# We'll then randomize the data, just to be sure not to get any pathological\n",
    "# ordering effects that might harm the performance of Stochastic Gradient\n",
    "# Descent.\n",
    "car_data = car_data.reindex(np.random.permutation(car_data.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VPUHcV0vuOAW"
   },
   "source": [
    "## Converting missing numerical values to the column mean\n",
    "\n",
    "When training a linear model using numerical features, we cannot have missing entries or NaN (doing so would cause overflow when training). Here we replace NaN (which corresponding to where we had missing entries) by the column mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1526161200590,
     "user": {
      "displayName": "Andre Cianflone",
      "photoUrl": "//lh6.googleusercontent.com/-tpiac-9KVF0/AAAAAAAAAAI/AAAAAAAACos/1A-E9qB_NVk/s50-c-k-no/photo.jpg",
      "userId": "107300886545482339938"
     },
     "user_tz": 240
    },
    "id": "WJg129wyugzF",
    "outputId": "985c9e7a-71e2-4742-bf5e-6f662aa11e18"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symboling</th>\n",
       "      <th>wheel-base</th>\n",
       "      <th>length</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>engine-size</th>\n",
       "      <th>compression-ratio</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>peak-rpm</th>\n",
       "      <th>city-mpg</th>\n",
       "      <th>highway-mpg</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>205.00</td>\n",
       "      <td>205.00</td>\n",
       "      <td>205.00</td>\n",
       "      <td>205.00</td>\n",
       "      <td>205.00</td>\n",
       "      <td>205.00</td>\n",
       "      <td>205.00</td>\n",
       "      <td>205.00</td>\n",
       "      <td>205.00</td>\n",
       "      <td>205.00</td>\n",
       "      <td>205.00</td>\n",
       "      <td>205.00</td>\n",
       "      <td>205.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.83</td>\n",
       "      <td>98.76</td>\n",
       "      <td>174.05</td>\n",
       "      <td>65.91</td>\n",
       "      <td>53.72</td>\n",
       "      <td>2555.57</td>\n",
       "      <td>126.91</td>\n",
       "      <td>10.14</td>\n",
       "      <td>104.26</td>\n",
       "      <td>5125.37</td>\n",
       "      <td>25.22</td>\n",
       "      <td>30.75</td>\n",
       "      <td>13207.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.25</td>\n",
       "      <td>6.02</td>\n",
       "      <td>12.34</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2.44</td>\n",
       "      <td>520.68</td>\n",
       "      <td>41.64</td>\n",
       "      <td>3.97</td>\n",
       "      <td>39.52</td>\n",
       "      <td>476.98</td>\n",
       "      <td>6.54</td>\n",
       "      <td>6.89</td>\n",
       "      <td>7868.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.00</td>\n",
       "      <td>86.60</td>\n",
       "      <td>141.10</td>\n",
       "      <td>60.30</td>\n",
       "      <td>47.80</td>\n",
       "      <td>1488.00</td>\n",
       "      <td>61.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>48.00</td>\n",
       "      <td>4150.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>5118.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00</td>\n",
       "      <td>94.50</td>\n",
       "      <td>166.30</td>\n",
       "      <td>64.10</td>\n",
       "      <td>52.00</td>\n",
       "      <td>2145.00</td>\n",
       "      <td>97.00</td>\n",
       "      <td>8.60</td>\n",
       "      <td>70.00</td>\n",
       "      <td>4800.00</td>\n",
       "      <td>19.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>7788.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.00</td>\n",
       "      <td>97.00</td>\n",
       "      <td>173.20</td>\n",
       "      <td>65.50</td>\n",
       "      <td>54.10</td>\n",
       "      <td>2414.00</td>\n",
       "      <td>120.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>95.00</td>\n",
       "      <td>5200.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>10595.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.00</td>\n",
       "      <td>102.40</td>\n",
       "      <td>183.10</td>\n",
       "      <td>66.90</td>\n",
       "      <td>55.50</td>\n",
       "      <td>2935.00</td>\n",
       "      <td>141.00</td>\n",
       "      <td>9.40</td>\n",
       "      <td>116.00</td>\n",
       "      <td>5500.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>34.00</td>\n",
       "      <td>16500.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.00</td>\n",
       "      <td>120.90</td>\n",
       "      <td>208.10</td>\n",
       "      <td>72.30</td>\n",
       "      <td>59.80</td>\n",
       "      <td>4066.00</td>\n",
       "      <td>326.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>288.00</td>\n",
       "      <td>6600.00</td>\n",
       "      <td>49.00</td>\n",
       "      <td>54.00</td>\n",
       "      <td>45400.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       symboling  wheel-base  length  width  height  weight  engine-size  \\\n",
       "count     205.00      205.00  205.00 205.00  205.00  205.00       205.00   \n",
       "mean        0.83       98.76  174.05  65.91   53.72 2555.57       126.91   \n",
       "std         1.25        6.02   12.34   2.15    2.44  520.68        41.64   \n",
       "min        -2.00       86.60  141.10  60.30   47.80 1488.00        61.00   \n",
       "25%         0.00       94.50  166.30  64.10   52.00 2145.00        97.00   \n",
       "50%         1.00       97.00  173.20  65.50   54.10 2414.00       120.00   \n",
       "75%         2.00      102.40  183.10  66.90   55.50 2935.00       141.00   \n",
       "max         3.00      120.90  208.10  72.30   59.80 4066.00       326.00   \n",
       "\n",
       "       compression-ratio  horsepower  peak-rpm  city-mpg  highway-mpg    price  \n",
       "count             205.00      205.00    205.00    205.00       205.00   205.00  \n",
       "mean               10.14      104.26   5125.37     25.22        30.75 13207.13  \n",
       "std                 3.97       39.52    476.98      6.54         6.89  7868.77  \n",
       "min                 7.00       48.00   4150.00     13.00        16.00  5118.00  \n",
       "25%                 8.60       70.00   4800.00     19.00        25.00  7788.00  \n",
       "50%                 9.00       95.00   5200.00     24.00        30.00 10595.00  \n",
       "75%                 9.40      116.00   5500.00     30.00        34.00 16500.00  \n",
       "max                23.00      288.00   6600.00     49.00        54.00 45400.00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_data['price'] = pd.to_numeric(car_data['price'], errors='coerce')\n",
    "car_data['horsepower'] = pd.to_numeric(car_data['horsepower'], errors='coerce')\n",
    "car_data['peak-rpm'] = pd.to_numeric(car_data['peak-rpm'], errors='coerce')\n",
    "car_data['city-mpg'] = pd.to_numeric(car_data['city-mpg'], errors='coerce')\n",
    "car_data['highway-mpg'] = pd.to_numeric(car_data['highway-mpg'], errors='coerce')\n",
    "\n",
    "# Replace nan by the mean storing the solution in the same table (`inplace')\n",
    "car_data.fillna(car_data.mean(), inplace=True)\n",
    "car_data.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "77Ag_VxFqCaX"
   },
   "source": [
    "## Setting Up the Feature Columns for TensorFlow\n",
    "In order to train a model in TensorFlow there is some set-up that will be needed each time. Each feature that you want to use for training will be put in what is called a feature column.  There are two kinds of basic data that you will use.\n",
    "\n",
    "*  ** Categorical Data** - This is data that is textual, for example `fuel-type` and `num-doors`\n",
    "*   **Numerical Data** - This is data that is a number (integer or float) that you want to treat as a number.  As we will discuss more later, sometimes you might want to treat numerical data (e.g., a zipcode) as if it were categorical.\n",
    "\n",
    "The first thing we will do is create a list of these type of features.  It is okay, as seen below, if one of these lists is empty.  In fact, we won't introduce any categorical data until a few labs later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "QnU22GBnr_i3"
   },
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = []\n",
    "NUMERICAL_COLUMNS = [\"price\", \"horsepower\", \"city-mpg\", \"highway-mpg\",\n",
    "                     \"peak-rpm\", \"compression-ratio\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2fUDFWSBrixq"
   },
   "source": [
    "## Input Function\n",
    "This next code block is some of the most complex code that will be provided. Please try to understand it the best you can.  However, you will not need to modify this so don't worry if you are confused a bit by it.\n",
    "\n",
    "\n",
    "The `input_fn` sets up a dictionary (hash map) needed to include numerical and categorical columns and assign each feature column to a unique integer id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "D1QHm1pkrjCS"
   },
   "outputs": [],
   "source": [
    "def input_fn(dataframe):\n",
    "  \"\"\"Constructs a dictionary for the feature columns\n",
    "\n",
    "  Args:\n",
    "    dataframe: The Pandas dataframe to use for the input.\n",
    "  Returns:\n",
    "    The feature columns and the associated labels for the provided input.\n",
    "  \"\"\"\n",
    "  # Creates a dictionary mapping from each numeric feature column name (k) to\n",
    "  # the values of that column stored in a constant Tensor.\n",
    "  numerical_cols = {k: tf.constant(dataframe[k].values)\n",
    "                     for k in NUMERICAL_COLUMNS}\n",
    "  # Creates a dictionary mapping from each categorical feature column name (k)\n",
    "  # to the values of that column stored in a tf.SparseTensor.\n",
    "  categorical_cols = {k: tf.SparseTensor(\n",
    "      indices=[[i, 0] for i in range(dataframe[k].size)],\n",
    "      values=dataframe[k].values,\n",
    "      dense_shape=[dataframe[k].size, 1])\n",
    "                      for k in CATEGORICAL_COLUMNS}\n",
    "  # Merges the two dictionaries into one.\n",
    "  feature_cols = dict(numerical_cols.items() + categorical_cols.items())\n",
    "  # Converts the label column into a constant Tensor.\n",
    "  label = tf.constant(dataframe[LABEL].values)\n",
    "  # Returns the feature columns and the label.\n",
    "  return feature_cols, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8l-FtgnwsxM1"
   },
   "source": [
    "## Specialize the Input Function for Training\n",
    "We will want to have different versions of the input function based on which dataset\n",
    "they are using.  For this lab we will just need a single input function\n",
    "(`train_input_fn`) that uses the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "r4KCc8L7sxVX"
   },
   "outputs": [],
   "source": [
    "def train_input_fn():\n",
    "  \"\"\"Sets up the input function using the training data.\n",
    "\n",
    "  Returns:\n",
    "     The feature columns to use for training and the associated labels.\n",
    "  \"\"\"\n",
    "  return input_fn(training_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AoFH5cbnuJsG"
   },
   "source": [
    "##Prepare Features\n",
    "As our learning models get more sophisticated we will want to do some computation on the features and even generate new features from the existing features. We will see examples of this in later labs.  For now this method will just make a copy of a portion of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "4_uZ7UirAw28"
   },
   "outputs": [],
   "source": [
    "def prepare_features(dataframe):\n",
    "  \"\"\"Prepares the features for provided dataset.\n",
    "\n",
    "  Args:\n",
    "    dataframe: A Pandas DataFrame expected to contain data from the\n",
    "      desired data set.\n",
    "  Returns:\n",
    "    A new dataFrame that contains the features to be used for the model.\n",
    "  \"\"\"\n",
    "  processed_features = dataframe.copy()\n",
    "  return processed_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zMB1h5gRu2_2"
   },
   "source": [
    "##Generate the Training Examples\n",
    "We simply call `prepare_features` on the `car_data` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "2vyxFjtnBRrx"
   },
   "outputs": [],
   "source": [
    "training_examples = prepare_features(car_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_examples = training_examples.copy()\n",
    "training_examples.update(training_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Trig1N4yFpJ"
   },
   "source": [
    "##Define the Numerical Feature Columns for TensorFlow\n",
    "\n",
    "We use [`tf.contrib.layers.real_valued_column`](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/real_valued_column) to define each column of numerical data.  We will introduce categorical data in a later lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "r_WBlyveQ38f"
   },
   "outputs": [],
   "source": [
    "def construct_feature_columns():\n",
    "  \"\"\"Construct TensorFlow Feature Columns for features\n",
    "  \n",
    "  Returns:\n",
    "    A set of feature columns\n",
    "  \"\"\"\n",
    "  feature_set = set([tf.contrib.layers.real_valued_column(feature) \n",
    "                     for feature in NUMERICAL_FEATURES])\n",
    "  return feature_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NRgIG5ITxcVi"
   },
   "source": [
    "\n",
    "## Setting up TensorFlow for Training a Basic Linear Regression Model\n",
    "\n",
    "\n",
    "The learning model we will use is [`tf.contrib.learn.LinearRegressor`](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/LinearRegressor).\n",
    "provided by the TensorFlow [`contrib.learn`](https://www.tensorflow.org/get_started/tflearn) library. This library helps make it easy to many things such as using Pandas columns as features, interacting with data, training models, and computing the predictions from a trained model.\n",
    "\n",
    "We use [`tf.train.GradientDescentOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer) as our algorithm to do the optimization. The GradientDescentOptimizer implements Stochastic Gradient Descent (SGD). The `learning_rate` parameter to the optimizer: it controls the size of the gradient step. We also include a value for `gradient_clip_norm` for safety. This makes sure that gradients are never too huge, which helps avoid pathological cases in gradient descent.\n",
    "\n",
    "You should take a little time reading the documentation for each of these methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "test": {
      "output": "ignore",
      "timeout": 600
     }
    },
    "colab_type": "code",
    "id": "Wvc1-JAhu6Fj",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def define_linear_regression_model(learning_rate):\n",
    "  \"\"\" Defines a linear regression model of one feature to predict the target.\n",
    "  \n",
    "  Args:\n",
    "    learning_rate: A `float`, the learning rate\n",
    "    \n",
    "  Returns:\n",
    "    A linear regressor crated with the given parameters\n",
    "  \"\"\"\n",
    "  linear_regressor = tf.contrib.learn.LinearRegressor(\n",
    "    feature_columns=construct_feature_columns(),\n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=learning_rate),\n",
    "    gradient_clip_norm=5.0\n",
    "  )  \n",
    "  return linear_regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wgu0umejzqNa"
   },
   "source": [
    "##Training a Model\n",
    "\n",
    "Now we can simply call the `fit` method to train our model. You might find this [documentation](https://www.tensorflow.org/extend/estimators) useful to read when you have questions about TensorFlow estimators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "wJxcDWfPxZB8"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'dict_items' and 'dict_items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-099423e93dbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Train the predictor using 1000 steps through the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_regressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m                 instructions)\n\u001b[0;32m--> 432\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    434\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, input_fn, steps, batch_size, monitors, max_steps)\u001b[0m\n\u001b[1;32m    522\u001b[0m       \u001b[0mhooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasic_session_run_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStopAtStepHook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks)\u001b[0m\n\u001b[1;32m   1036\u001b[0m       \u001b[0mrandom_seed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_random_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m       \u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m       \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m       \u001b[0mtraining_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_or_create_global_step_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-1b3658d2b181>\u001b[0m in \u001b[0;36mtrain_input_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m      \u001b[0mThe\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0mto\u001b[0m \u001b[0muse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0massociated\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \"\"\"\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-b18282a02e75>\u001b[0m in \u001b[0;36minput_fn\u001b[0;34m(dataframe)\u001b[0m\n\u001b[1;32m     19\u001b[0m                       for k in CATEGORICAL_COLUMNS}\n\u001b[1;32m     20\u001b[0m   \u001b[0;31m# Merges the two dictionaries into one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m   \u001b[0mfeature_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumerical_cols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcategorical_cols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m   \u001b[0;31m# Converts the label column into a constant Tensor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLABEL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'dict_items' and 'dict_items'"
     ]
    }
   ],
   "source": [
    "NUMERICAL_FEATURES = [\"horsepower\"]\n",
    "CATEGORICAL_FEATURES = []\n",
    "LABEL = \"price\"\n",
    "\n",
    "# Create regression model using the define_regression_model procedure that we\n",
    "# defined earlier.\n",
    "linear_regressor = define_linear_regression_model(learning_rate = 0.01)\n",
    "\n",
    "# Train the predictor using 1000 steps through the data.\n",
    "_ = linear_regressor.fit(input_fn=train_input_fn, steps=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1aHq273-7EfE"
   },
   "source": [
    "## Looking at the feature weight (slope) and bias of our trained model\n",
    "\n",
    "TensorFlow provides an easy way to view the weights of the trained model. Although we just have a single feature right now, this code block shows how you could access and print all of the feature weights for a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 243,
     "status": "ok",
     "timestamp": 1526161205705,
     "user": {
      "displayName": "Andre Cianflone",
      "photoUrl": "//lh6.googleusercontent.com/-tpiac-9KVF0/AAAAAAAAAAI/AAAAAAAACos/1A-E9qB_NVk/s50-c-k-no/photo.jpg",
      "userId": "107300886545482339938"
     },
     "user_tz": 240
    },
    "id": "Ip8m7zFU7ES9",
    "outputId": "c48631b6-fb51-4ba1-d31a-5300be8617fd"
   },
   "outputs": [],
   "source": [
    "w = linear_regressor.get_variable_value('linear/horsepower/weight')[0]\n",
    "print 'slope', w\n",
    "b = linear_regressor.get_variable_value('linear/bias_weight')[0]\n",
    "print 'bias', b  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3aBE_yqlCyHR"
   },
   "source": [
    "## Getting Predictions For a Trained Model\n",
    "Once we have trained a model we can call `predict` to get a list of predictions on the data provided via `train_input_fn`.  For now we will do that for the training data and show how we can then compute the loss (RMSE) on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "DtYxaC5Zu6aO"
   },
   "outputs": [],
   "source": [
    "predictions = list(linear_regressor.predict(input_fn=train_input_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U7qT33YbG3Jf"
   },
   "source": [
    "## Showing Our Trained Model in a Scatter Plot\n",
    "When training a linear regression model over a single variable, a really nice thing to be able to do is to show the model (which is just a line) as part of the scatter plot. That really helps you see how well the model fits the data.  Just looking at the loss (RMSE here) doesn't really indicate how good the model is.  Sometimes you want to show several models on the same scatter plot to compare them so we allow `slopes`, `biases`, and `model_names` to all be lists.  They should be of the same size giving the weight (slope), bias, and name (to use in the legend) for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "uTzgIq_hZL0m"
   },
   "outputs": [],
   "source": [
    "def make_scatter_plot(dataframe, input_feature, target,\n",
    "                      slopes=[], biases=[], model_names=[]):\n",
    "  \"\"\" Creates a scatter plot of input_feature vs target along with the models.\n",
    "  \n",
    "  Args:\n",
    "    dataframe: the dataframe to visualize\n",
    "    input_feature: the input feature to be used for the x-axis\n",
    "    target: the target to be used for the y-axis\n",
    "    slopes: list of model weight (slope) \n",
    "    bias: list of model bias (same size as slopes)\n",
    "    model_names: list of model_names to use for legend (same size as slopes)\n",
    "  \"\"\"      \n",
    "  # Define some colors to use that go from blue towards red\n",
    "  colors = [cm.coolwarm(x) for x in np.linspace(0, 1, len(slopes))]\n",
    "  \n",
    "  # Generate the scatter plot\n",
    "  x = dataframe[input_feature]\n",
    "  y = dataframe[target]\n",
    "  plt.ylabel(target)\n",
    "  plt.xlabel(input_feature)\n",
    "  plt.scatter(x, y, color='black', label=\"\")\n",
    "\n",
    "  # Add the lines corresponding to the provided models\n",
    "  for i in range (0, len(slopes)):\n",
    "    y_0 = slopes[i] * x.min() + biases[i]\n",
    "    y_1 = slopes[i] * x.max() + biases[i]\n",
    "    plt.plot([x.min(), x.max()], [y_0, y_1],\n",
    "             label=model_names[i], color=colors[i])\n",
    "  plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NTCO0kbnuxxH"
   },
   "source": [
    "##Calibration Plot###\n",
    "When we just use a single input feature, we can visualize the data and the learned model pretty well.  In order to help understand higher-dimensional models, a **calibration plot** is very useful. A calibration plot is similar to a scatter plot of the data except this can be used for any linear model since it just plots the target with respec to the predictions. Observe that a model with an RMSE of 0 would have all points on the line target = prediction.  For points that are under the line, we are overpredicting and points over the line we are underpredicting.  This method shows the line target = prediction to help visualize how well the model is doing.\n",
    "\n",
    "Observe that unlike in the scatter plot where the points are fixed and the model (as viewed as a line changes), in the callibration plot the x-coordinate of the points change as the model is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "yjFZ_FXFvTgJ"
   },
   "outputs": [],
   "source": [
    "def make_calibration_plot(predictions, targets):\n",
    "  \"\"\" Creates a calibration plot.\n",
    "  \n",
    "  Args:\n",
    "    predictions: a list of values predicted by the model being visualized\n",
    "    targets: a list of the target values being predicted that must be the\n",
    "             same size as predictions.\n",
    "  \"\"\"  \n",
    "  calibration_data = pd.DataFrame()\n",
    "  calibration_data[\"predictions\"] = pd.Series(predictions)\n",
    "  calibration_data[\"targets\"] = pd.Series(targets)\n",
    "  calibration_data.describe()\n",
    "  min_val = calibration_data[\"predictions\"].min()\n",
    "  max_val = calibration_data[\"predictions\"].max()\n",
    "  plt.ylabel(\"target\")\n",
    "  plt.xlabel(\"prediction\")\n",
    "  plt.scatter(predictions, targets, color='black')\n",
    "  plt.plot([min_val, max_val], [min_val, max_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T_fieEgPEbmI"
   },
   "source": [
    "Notice that the model we have trained so far is not very good and you can see this when looking at the calibration plot. Your first task will be to improve this model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_qbFEC4SJBty"
   },
   "source": [
    "## Task 1 (1 point)\n",
    "\n",
    "The code box below generates a calibration plot for this model.  How does it look compared to the scatter plot?  Explain what you are seeing and why it is this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2687,
     "status": "ok",
     "timestamp": 1526161210273,
     "user": {
      "displayName": "Andre Cianflone",
      "photoUrl": "//lh6.googleusercontent.com/-tpiac-9KVF0/AAAAAAAAAAI/AAAAAAAACos/1A-E9qB_NVk/s50-c-k-no/photo.jpg",
      "userId": "107300886545482339938"
     },
     "user_tz": 240
    },
    "id": "6M3_9N-cKANp",
    "outputId": "c070de7e-56bf-4ff2-d0e0-a00ed9969c04"
   },
   "outputs": [],
   "source": [
    "dataframe = car_data\n",
    "input_feature = \"horsepower\"\n",
    "target = \"price\"\n",
    "\n",
    "# Train the predictor using 1000 steps through the data.\n",
    "linear_regressor1 = define_linear_regression_model(learning_rate = 0.01)\n",
    "_ = linear_regressor1.fit(\n",
    "      input_fn=train_input_fn, steps=1500\n",
    ")\n",
    "w1 = linear_regressor1.get_variable_value('linear/horsepower/weight')[0]\n",
    "b1 = linear_regressor1.get_variable_value('linear/bias_weight')[0] \n",
    "\n",
    "# Train the predictor using 10 steps through the data.\n",
    "linear_regressor2 = define_linear_regression_model(learning_rate = 0.01)\n",
    "_ = linear_regressor2.fit(\n",
    "      input_fn=train_input_fn, steps=1000\n",
    ")\n",
    "w2 = linear_regressor2.get_variable_value('linear/horsepower/weight')[0]\n",
    "b2 = linear_regressor2.get_variable_value('linear/bias_weight')[0] \n",
    "\n",
    "make_scatter_plot(dataframe, input_feature, target,\n",
    "                      slopes=[w1,w2], biases=[b1,b2], model_names=[\"model1\",\"model2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 402,
     "status": "ok",
     "timestamp": 1526161210755,
     "user": {
      "displayName": "Andre Cianflone",
      "photoUrl": "//lh6.googleusercontent.com/-tpiac-9KVF0/AAAAAAAAAAI/AAAAAAAACos/1A-E9qB_NVk/s50-c-k-no/photo.jpg",
      "userId": "107300886545482339938"
     },
     "user_tz": 240
    },
    "id": "nzQ2_AH3Me5d",
    "outputId": "3d0d3988-f007-4012-c729-741a520b29c3"
   },
   "outputs": [],
   "source": [
    "make_calibration_plot(predictions, car_data[LABEL].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AZWF67uv0HTG",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing the Loss\n",
    "For now we are using root mean squared error (RMSE) for our loss since that is the appropriate loss to use for linear regression.  However, to keep the procedure to train the model very generic, we will use a method compute loss that can be tailored to other types of problems. For this lab, our implementation will be to return the RMSE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "pZ8hg8DeAjOe"
   },
   "outputs": [],
   "source": [
    "def compute_loss(predictions, targets):\n",
    "  \"\"\" Computes the loss (RMSE) for linear regression.\n",
    "  \n",
    "  Args:\n",
    "    predictions: a list of values predicted by the model being visualized\n",
    "    targets: a list of the target values being predicted that must be the\n",
    "             same size as predictions.\n",
    "    \n",
    "  Returns:\n",
    "    The RMSE for the provided predictions and targets\n",
    "  \"\"\"      \n",
    "  return math.sqrt(metrics.mean_squared_error(predictions, targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gCv6ZQL2GMaN"
   },
   "source": [
    "##Learning Curve\n",
    "\n",
    "Another important tool is a graph often called a **learning curve** that shows the loss being minimized on the y-axis and the training steps (time) on the x-axis.  Looking at the learning curve will help you understand if you have set the `learning_rate` too high or too low, how many `steps` you need to train and other things that we'll explore later.  \n",
    "\n",
    "Next we define a method to plot the learning curve given a list of training losses that will be recorded at regular intervals during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "C6rlWwMpOG0D"
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(training_losses):\n",
    "  \"\"\" Plot the learning curve\n",
    "  \n",
    "  Args:\n",
    "    training_loses: a list of losses to plot\n",
    "  \"\"\"        \n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Training Steps')\n",
    "  plt.plot(training_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HYwpTeMeHT9D"
   },
   "source": [
    "##Training Our Model\n",
    "\n",
    "We now have all the pieces we need to train a model.  In order to generate intermediate losses for the learning curve (and record as we are training), we divide the training into 10 periods.  After each period we compute the loss.  We also store the weight and bias of the model at that time so that we can then visually show how the model evolves in a scatter plot.  You are welcome to modify the number of periods but 10 seems to work out pretty well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Ivp8BYVNMFT-"
   },
   "outputs": [],
   "source": [
    "def train_model(linear_regressor, steps):\n",
    "  \"\"\"Trains a linear regression model.\n",
    "  \n",
    "  Args:\n",
    "    linear_regressor: The regressor to train\n",
    "    steps: A non-zero `int`, the total number of training steps.\n",
    "    \n",
    "  Returns:\n",
    "    The trained regressor\n",
    "  \"\"\"\n",
    "  # In order to see how the model evolves as we train it, we will divide the\n",
    "  # steps into periods and show the model after each period.\n",
    "  periods = 10\n",
    "  steps_per_period = steps / periods\n",
    "  \n",
    "  # Train the model, but do so inside a loop so that we can periodically assess\n",
    "  # loss metrics.  We store the loss, slope (feature weight), bias, and a name\n",
    "  # for the model when there is a single feature (which would then allos us\n",
    "  # to plot the model in a scatter plot).\n",
    "  print \"Training model...\"\n",
    "  training_losses = []\n",
    "  slopes = []\n",
    "  biases = []\n",
    "  model_names = []\n",
    "\n",
    "  for period in range (0, periods):\n",
    "    # Call fit to train the regressor for steps_per_period steps\n",
    "    linear_regressor.fit(input_fn=train_input_fn, steps=steps_per_period)\n",
    "\n",
    "    # Use the predict method to compute the predictions from the current model\n",
    "    predictions = np.array(list(linear_regressor.predict(\n",
    "        input_fn=train_input_fn)))\n",
    "   \n",
    "    # Compute the loss between the predictions and the correct labels, append\n",
    "    # the loss to the list of losses used to generate the learning curve after\n",
    "    # training is complete and print the current loss\n",
    "    loss = compute_loss(predictions, training_examples[LABEL])\n",
    "    training_losses.append(loss) \n",
    "    print \"  Loss after period %02d : %0.3f\" % (period, loss)\n",
    "     \n",
    "    # When there is a single input feature, add slope, bias and model_name to\n",
    "    # the lists to be used later to plot the model after each training period.\n",
    "    if len(NUMERICAL_FEATURES) == 1 and len(CATEGORICAL_FEATURES) == 0:\n",
    "      feature_weight = fnmatch.filter(linear_regressor.get_variable_names(),\n",
    "                                      'linear/*/weight')\n",
    "      slopes.append(linear_regressor.get_variable_value(\n",
    "          feature_weight[0])[0])\n",
    "      biases.append(linear_regressor.get_variable_value(\n",
    "          'linear/bias_weight')[0])\n",
    "      model_names.append(\"period_\" + str(period))\n",
    "      \n",
    "  # Now that training is done print the final loss    \n",
    "  print \"Final Loss (RMSE) on the training data: %0.3f\" % loss \n",
    "  \n",
    "  # Generate a figure with the learning curve on the left and either a scatter\n",
    "  # plot or calibration plot (when more than 2 input features) on the right\n",
    "  plt.figure(figsize=(10, 5))\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.title(\"Learning Curve (RMSE vs time)\")\n",
    "  plot_learning_curve(training_losses)\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.tight_layout(pad=1.1, w_pad=3.0, h_pad=3.0)\n",
    " \n",
    "  if len(NUMERICAL_FEATURES) > 1 or len(CATEGORICAL_FEATURES) != 0:\n",
    "    plt.title(\"Calibration Plot\")\n",
    "    make_calibration_plot(predictions, training_examples[LABEL])\n",
    "\n",
    "  else:\n",
    "    plt.title(\"Learned Model by Period on Scatter Plot\")\n",
    "    make_scatter_plot(training_examples, NUMERICAL_FEATURES[0], LABEL,\n",
    "                      slopes, biases, model_names)\n",
    "   \n",
    "  return linear_regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cZMZqALDrH2e"
   },
   "source": [
    "### Example learning curve when the learning rate that is too high\n",
    "\n",
    "When the learning rate is too high you will see the loss going up and down indicating you are making adjustments that are too large.  When you see this happening lower the learning rate (initially by a factor of 10 and then make smaller adjustments when you are close).  In this case you are moving back and forth between having the slope too large and too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 576
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5769,
     "status": "ok",
     "timestamp": 1526161218508,
     "user": {
      "displayName": "Andre Cianflone",
      "photoUrl": "//lh6.googleusercontent.com/-tpiac-9KVF0/AAAAAAAAAAI/AAAAAAAACos/1A-E9qB_NVk/s50-c-k-no/photo.jpg",
      "userId": "107300886545482339938"
     },
     "user_tz": 240
    },
    "id": "MS1WGzxxrHUn",
    "outputId": "f719cd37-6302-4f23-c661-ddc7ddf2a781"
   },
   "outputs": [],
   "source": [
    "NUMERICAL_FEATURES = [\"horsepower\"]\n",
    "CATEGORICAL_FEATURES = []\n",
    "LABEL = \"price\"\n",
    "\n",
    "LEARNING_RATE = 100\n",
    "STEPS = 50\n",
    "\n",
    "linear_regressor = define_linear_regression_model(learning_rate = LEARNING_RATE)\n",
    "linear_regressor = train_model(linear_regressor, steps=STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oHRxZe5xrHCo"
   },
   "source": [
    "### Example learning curve when the learning rate that is too low\n",
    "\n",
    "When the learning rate is too low then the changes are too small.  While this might eventually get you to a good solution it would take way more steps than needed and the training time is roughly proportinal to the number of steps so you want to find a learning rate that gets you to a good solution as fast as you can.  You can see for these settings that the model learned (the line you see in the scatter plot) is improving and would eventually get there but is taking much, much longer than needed to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 576
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10618,
     "status": "ok",
     "timestamp": 1526161229147,
     "user": {
      "displayName": "Andre Cianflone",
      "photoUrl": "//lh6.googleusercontent.com/-tpiac-9KVF0/AAAAAAAAAAI/AAAAAAAACos/1A-E9qB_NVk/s50-c-k-no/photo.jpg",
      "userId": "107300886545482339938"
     },
     "user_tz": 240
    },
    "id": "VnZwKYkxrziv",
    "outputId": "908723ba-c60f-4849-c1dc-bdac7f63d33a"
   },
   "outputs": [],
   "source": [
    "NUMERICAL_FEATURES = [\"horsepower\"]\n",
    "CATEGORICAL_FEATURES = []\n",
    "LABEL = \"price\"\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "STEPS = 10000\n",
    "\n",
    "linear_regressor = define_linear_regression_model(learning_rate = LEARNING_RATE)\n",
    "linear_regressor = train_model(linear_regressor, steps=STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sCkq548MRWlK"
   },
   "source": [
    "##Task 2 - Modify the hyperparmaters to get a better model. (1 Points)\n",
    "For this task, you can use the code block below that puts all the above code in a single cell for convenience. Focus on first finding a good learning rate and then adjusting the number of steps to be what you need to converge.\n",
    "\n",
    "**IN ORDER TO GET CREDIT FOR THIS TASK, YOU MUST EDIT THIS TEXT BLOCK STARTING HERE TO ANSWER THESE QUESTIONS. PLEASE DO THIS THROUGHOUT ALL OF THE LABS WHEN ASKED A QUESTION IN A TEXT BOX**\n",
    "\n",
    "* List at least 3 of the sets of hyperparameters you tried and the RMSE obtained.  Your primary goal is to get the lowest RMSE you can.  Once you've done that a secondary goal is to minmize the number of steps used since the computation cost depends heavily on the number of steps.\n",
    "\n",
    "* Submit this with the results from the hyperparameters that you feel worked best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5394,
     "status": "ok",
     "timestamp": 1526161265380,
     "user": {
      "displayName": "Andre Cianflone",
      "photoUrl": "//lh6.googleusercontent.com/-tpiac-9KVF0/AAAAAAAAAAI/AAAAAAAACos/1A-E9qB_NVk/s50-c-k-no/photo.jpg",
      "userId": "107300886545482339938"
     },
     "user_tz": 240
    },
    "id": "rxUHBNcmMTzx",
    "outputId": "85cdb4ba-2503-43f8-fad1-b725a4ecf65f"
   },
   "outputs": [],
   "source": [
    "NUMERICAL_FEATURES = [\"horsepower\"]\n",
    "CATEGORICAL_FEATURES = []\n",
    "LABEL = \"price\"\n",
    "\n",
    "## Fill in the rest of your solution here.  Feel free to introduce multiple\n",
    "## code boxes if you want to see the solutions and learning curves from\n",
    "## different options at the same time\n",
    "# Train the predictor using 1000 steps through the data.\n",
    "linear_regressor3 = define_linear_regression_model(learning_rate = 0.01)\n",
    "_ = linear_regressor3.fit(\n",
    "      input_fn=train_input_fn, steps=10000\n",
    ")\n",
    "w3 = linear_regressor3.get_variable_value('linear/horsepower/weight')[0]\n",
    "b3 = linear_regressor3.get_variable_value('linear/bias_weight')[0] \n",
    "\n",
    "make_scatter_plot(dataframe, input_feature, target,\n",
    "                      slopes=[w3], biases=[b3], model_names=[\"good model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vg1rzJ4huIL9"
   },
   "source": [
    "**Put your answers the the given questions in this text box.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cWTvMXoJZOcP"
   },
   "source": [
    "##Task 3: Try a Different Input Feature. (3 Points)\n",
    "\n",
    "The choice of the hyperparameters depedns a lot on the data set and what you are trying to learn.  In this task you will try to predict the price from highway mpg. In this task you will find a good set of hyperparmeters for this problem.\n",
    "\n",
    "* Use highway-mpg instead of horsepower to predict price  You might want to start by just plotting the data.  What do you observe?\n",
    "* What hyperparameters give you the best trained model that you can get.  Try to keep the learning steps as small as you can while still training a good model.\n",
    "* Did you have to change the hyperparameters a lot?  If you did, why do you think that might be the case?\n",
    "* How does the RMSE for your model compare to the optimal RMSE?  Think about what you'll need to do in order to answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5650,
     "status": "ok",
     "timestamp": 1526161441103,
     "user": {
      "displayName": "Andre Cianflone",
      "photoUrl": "//lh6.googleusercontent.com/-tpiac-9KVF0/AAAAAAAAAAI/AAAAAAAACos/1A-E9qB_NVk/s50-c-k-no/photo.jpg",
      "userId": "107300886545482339938"
     },
     "user_tz": 240
    },
    "id": "0Py3f-_YYYc-",
    "outputId": "8f30ad4b-89d9-4d8b-8154-7a37df9592f8"
   },
   "outputs": [],
   "source": [
    "NUMERICAL_FEATURES = [\"highway-mpg\"]\n",
    "CATEGORICAL_FEATURES = []\n",
    "LABEL = \"price\"\n",
    "\n",
    "dataframe = car_data\n",
    "input_feature = \"highway-mpg\"\n",
    "target = \"price\"\n",
    "\n",
    "## Fill in the rest of your solution here.\n",
    "linear_regressor4 = define_linear_regression_model(learning_rate = 0.01)\n",
    "_ = linear_regressor4.fit(\n",
    "      input_fn=train_input_fn, steps=10000\n",
    ")\n",
    "w4 = linear_regressor4.get_variable_value('linear/highway-mpg/weight')[0]\n",
    "b4 = linear_regressor4.get_variable_value('linear/bias_weight')[0] \n",
    "\n",
    "make_scatter_plot(dataframe, input_feature, target,\n",
    "                      slopes=[w4], biases=[b4], model_names=[\"model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4rtaN2Hnt-LP"
   },
   "source": [
    "**Put your answers the the given questions in this text box.**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "JndnmDMp66FL"
   ],
   "default_view": {},
   "name": "Lab 2_solutions-Training Your First Linear Regression Model.ipynb",
   "provenance": [
    {
     "file_id": "/v2/external/notebooks/intro_to_ml_semester_course/Lab_2__Training_Your_First_TF_Linear_Regression_Model.ipynb",
     "timestamp": 1526151942360
    },
    {
     "file_id": "0BxDag_sVV0QbcXhCM1RQYTBiVnc",
     "timestamp": 1504122945138
    },
    {
     "file_id": "0BxDag_sVV0QbeHl6TnVGaERiRWs",
     "timestamp": 1504115870628
    },
    {
     "file_id": "0BxDag_sVV0QbM0RHakNBcGtFdlE",
     "timestamp": 1503350093341
    },
    {
     "file_id": "0BxDag_sVV0QbZnBLa1FqNEE3bTg",
     "timestamp": 1500399420475
    },
    {
     "file_id": "0BxDag_sVV0QbYVM3cV9XVWhrRjg",
     "timestamp": 1500234272739
    },
    {
     "file_id": "0B1ymeC_TeE1dQUdHcVpFbC10YmM",
     "timestamp": 1463102040711
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
